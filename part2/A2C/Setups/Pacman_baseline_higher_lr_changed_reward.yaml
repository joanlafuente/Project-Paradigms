seed: 0 # Seed for reproducibility
device: cuda
env_name: MsPacmanNoFrameskip-v4

pretrained: False # Set to True if you want to load a pretrained model
check_freq: 2000 # check_freq is the frequency at which the callback is called, in this case, the callback is called every 2000 timesteps
logs_dir: "./log_dir" # The directory for tensorboard logs

total_timesteps: 20000000 # Total training timesteps
log_interval: 10 # Interval between log entries

Environment:
  n_stack: 4 # Number of frames to stack
  n_envs: 16 # Number of parallel environments
  frame_skip: 4 # Number of frames to skip between each action

test_episodes: 100 # Number of episodes to test the model

ModelParams:
  policy: 'CnnPolicy' 
  policy_kwargs: "dict(optimizer_class=RMSpropTFLike, optimizer_kwargs=dict(eps=1e-5))" # Best for A2C, condiering rl-baselines3-zoo

  gamma: 0.90 
  
  learning_rate: 0.001

  # Optimized hyperparameters from rl-baselines3-zoo for A2C
  ent_coef: 0.01 # Entropy coefficient for the loss calculation, 0 to disable
  vf_coef: 0.25 # Value function loss coefficient, If set to 0.5, value function loss will be half the policy loss

  stats_window_size: 100 # The size of the window used to compute the running average of the episode rewards, and std
  verbose: 2 # The verbosity level: 0 no output, 1 info, 2 debug

