{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supersuit as ss\n",
    "from pettingzoo.atari import pong_v3\n",
    "from pettingzoo.utils import agent_selector\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Single Match Video Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = pong_v3.env(num_players=2, render_mode=\"rgb_array\")\n",
    "\n",
    "# Pre-process using SuperSuit\n",
    "env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 4, stack_dim=0)\n",
    "env = ss.dtype_v0(env, dtype=np.float32)\n",
    "env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "\n",
    "# Load the agents\n",
    "model1 = PPO.load(\"/home/joan/Desktop/Runs/Pong_baseline_adversarial_v5/Pong_baseline_adversarial_v5_0.zip\")\n",
    "model2 = PPO.load(\"/home/joan/Desktop/Runs/Pong_baseline_adversarial_v5/Pong_baseline_adversarial_v5_1.zip\")\n",
    "\n",
    "rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "# We evaluate here using an AEC environments\n",
    "env.reset(seed=1234)\n",
    "env.action_space(env.possible_agents[0]).seed(0)\n",
    "\n",
    "# List of images to create a gif\n",
    "images = []\n",
    "for agent in env.agent_iter():\n",
    "    # Getting the observation and action space\n",
    "    obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    # Update the rewards\n",
    "    for a in env.agents:\n",
    "        rewards[a] += env.rewards[a]\n",
    "\n",
    "    # If the game is over, break\n",
    "    if termination or truncation:\n",
    "        break\n",
    "    else:\n",
    "        # Select the action\n",
    "        if agent == env.possible_agents[0]:\n",
    "            act = model1.predict(obs)[0]\n",
    "        else:\n",
    "            act = model2.predict(obs)[0]\n",
    "\n",
    "    # Perform the action\n",
    "    env.step(act)\n",
    "\n",
    "    # Store an image of the current state of the environment\n",
    "    images.append(env.render())\n",
    "\n",
    "    # If we have more than 100000 images, break\n",
    "    if len(images) > 100000:\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_0': -9, 'second_0': 9}\n"
     ]
    }
   ],
   "source": [
    "# Match rewards\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and store them in a list\n",
    "images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "# Save as GIF\n",
    "images[0].save('output.gif', save_all=True, append_images=images[1:], duration=20, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 100\n",
    "# Create the environment\n",
    "env = pong_v3.env(num_players=2, render_mode=\"rgb_array\")\n",
    "\n",
    "# Pre-process using SuperSuit\n",
    "env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "env = ss.frame_stack_v1(env, 4, stack_dim=0)\n",
    "env = ss.dtype_v0(env, dtype=np.float32)\n",
    "env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
    "\n",
    "# Load the agents\n",
    "model1 = PPO.load(\"/home/joan/Desktop/Pong_baseline_adversarial_v5_0.zip\")\n",
    "model2 = PPO.load(\"/home/joan/Desktop/Pong_baseline_adversarial_v5_1.zip\")\n",
    "\n",
    "# Initialize the variables to store the results\n",
    "total_diff_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "total_wins = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "# We evaluate here using an AEC environments\n",
    "env.reset(seed=1234)\n",
    "env.action_space(env.possible_agents[0]).seed(0)\n",
    "\n",
    "# Iterate over the test episodes\n",
    "for i in range(test_episodes):\n",
    "    # Initialize the rewards for each agent\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    # Iterate over the agents\n",
    "    for agent in env.agent_iter():\n",
    "        # Getting the observation\n",
    "        obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        # Update the rewards\n",
    "        for a in env.agents:\n",
    "            rewards[a] += env.rewards[a]\n",
    "\n",
    "        # If the game is over, break\n",
    "        if termination or truncation:\n",
    "            # Update the accumulated rewards\n",
    "            for a in env.agents:\n",
    "                total_diff_rewards[a] += rewards[a]\n",
    "\n",
    "            # Update the total points scored and wins\n",
    "            a1, a2 = env.agents\n",
    "            if rewards[a1] > rewards[a2]:\n",
    "                total_rewards[a1] += 21\n",
    "                total_rewards[a2] += (21 + rewards[a2])\n",
    "                total_wins[a1] += 1\n",
    "            else:\n",
    "                total_rewards[a2] += 21\n",
    "                total_rewards[a1] += (21 + rewards[a1])\n",
    "                total_wins[a2] += 1    \n",
    "\n",
    "            # Reset the environment and break the loop of the episode  \n",
    "            env.reset()\n",
    "            break\n",
    "        else:\n",
    "            # Select the action\n",
    "            if agent == env.possible_agents[0]:\n",
    "                act = model1.predict(obs)[0]\n",
    "            else:\n",
    "                act = model2.predict(obs)[0]\n",
    "        env.step(act)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_0': 1492, 'second_0': 2050}\n",
      "{'first_0': -558, 'second_0': 558}\n",
      "{'first_0': 10, 'second_0': 90}\n"
     ]
    }
   ],
   "source": [
    "# Points made by each agent\n",
    "print(total_rewards)\n",
    "# Accumulated rewards of each agent for the 100 episodes\n",
    "print(total_diff_rewards)\n",
    "# Number of wins of each agent\n",
    "print(total_wins)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
