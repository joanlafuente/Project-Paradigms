{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import collections\n",
    "\n",
    "\n",
    "import random\n",
    "import wandb\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in /Users/jlafuente/Desktop/Unet/.conda/lib/python3.12/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>1.20 in /Users/jlafuente/Desktop/Unet/.conda/lib/python3.12/site-packages (from ale-py) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Install the ALE package\n",
    "!pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"ALE/Breakout-v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160)\n",
      "MaxAndSkipObservation: (210, 160)\n",
      "ResizeObservation    : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n",
      "\n",
      "Action space is Discrete(4) \n",
      "Observation space is Box(0, 255, (4, 84, 84), uint8) \n"
     ]
    }
   ],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name, obs_type=\"grayscale\")\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipObservation(env, skip=4)\n",
    "    print(\"MaxAndSkipObservation: {}\".format(env.observation_space.shape))\n",
    "    #env = FireResetEnv(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    print(\"FrameStackObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "    \n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(ENV_NAME)\n",
    "print(\"\\nAction space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=5, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1600, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value_prediction = nn.Linear(512, 1)\n",
    "        self.advantage_prediction = nn.Linear(512, output_shape)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.net(x)\n",
    "        value = self.value_prediction(embedding)\n",
    "        advantage = self.advantage_prediction(embedding)\n",
    "        q_values = value + advantage - advantage.mean(dim=-1).unsqueeze(-1)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_replay_buffer):\n",
    "        self.env = env\n",
    "        self.exp_replay_buffer = exp_replay_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.current_state = self.env.reset()[0]\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, net, target_net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_ = np.array([self.current_state])\n",
    "            state = torch.tensor(state_).to(device)\n",
    "            q_vals = net(state)\n",
    "            _, act_ = torch.max(q_vals, dim=1)\n",
    "            action = int(act_.item())\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        is_done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.current_state, action, reward, is_done, new_state)\n",
    "        self.exp_replay_buffer.append(exp, net, target_net, device)\n",
    "        self.current_state = new_state\n",
    "        \n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        \n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, target_model, states, actions, rewards, dones, next_states, gamma=0.99, criterion=nn.MSELoss()):\n",
    "    Q_values = model(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    next_state_values = target_model(next_states).max(1)[0]\n",
    "    next_state_values[dones] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_Q_values = next_state_values * gamma + rewards\n",
    "\n",
    "    return criterion(Q_values, expected_Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_error(model, target_model, states, actions, rewards, dones, next_states, gamma=0.99, device=\"cpu\"):\n",
    "    states = torch.tensor(states).to(device)\n",
    "    actions = torch.tensor(actions).to(device)\n",
    "    rewards = torch.tensor(rewards).to(device)\n",
    "    dones = torch.tensor(dones).to(device)\n",
    "    next_states = torch.tensor(next_states).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Q_values = model(states.unsqueeze(0)).gather(1, actions.unsqueeze(0).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        next_state_values = target_model(next_states.unsqueeze(0)).max(1)[0]\n",
    "        next_state_values[dones] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_Q_values = next_state_values * gamma + rewards\n",
    "        \n",
    "    return (Q_values - expected_Q_values).abs().detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedExperienceReplay:\n",
    "    \"\"\"\n",
    "    D'aquest metode no se si es correcte del tot, en comptes de calcular el \n",
    "    td_error quan faig el sampling ho he implementat en el moment en que safegeix al buffer.\n",
    "\n",
    "    D'aquesta forma no augmenta molt tant al numero de claculs extra (Sino shauria de clacular \n",
    "    per a tot el buffer cada vegada que fem sampling)\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        self.priorities = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience, model, target_model, device=\"cpu\"):\n",
    "        td_error = compute_td_error(model, target_model, experience.state, experience.action, experience.reward, experience.done, experience.new_state, device=device)\n",
    "        self.priorities.append(td_error)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, BATCH_SIZE, alpha=0.6, beta=0.4, epsilon=0.01):\n",
    "        priorities = np.array(self.priorities)\n",
    "        priorities = priorities + epsilon\n",
    "        probabilities = priorities ** alpha\n",
    "        probabilities = probabilities / probabilities.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), BATCH_SIZE, p=probabilities)\n",
    "        weights = (1/len(self.buffer) * 1/probabilities[indices]) ** beta\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "\n",
    "        states = torch.from_numpy(np.array(states, dtype=np.float32))\n",
    "        actions = torch.from_numpy(np.array(actions, dtype=np.int64))\n",
    "        rewards = torch.from_numpy(np.array(rewards, dtype=np.float32))\n",
    "        dones = torch.from_numpy(np.array(dones, dtype=bool))\n",
    "        next_states = torch.from_numpy(np.array(next_states, dtype=np.float32))\n",
    "        weights = torch.from_numpy(np.array(weights, dtype=np.float32))\n",
    "\n",
    "        return states, actions, rewards, dones, next_states, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_REWARD_BOUND = 19.0 \n",
    "NUMBER_OF_REWARDS_TO_AVERAGE = 10          \n",
    "\n",
    "GAMMA = 0.99       \n",
    "\n",
    "BATCH_SIZE = 32  \n",
    "LEARNING_RATE = 1e-4           \n",
    "\n",
    "EXPERIENCE_REPLAY_SIZE = 10000            \n",
    "SYNC_TARGET_NETWORK = 1000     \n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.999985\n",
    "EPS_MIN = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lxbjyxlr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>loss</td><td>▃▁▃▁▂▂▂▁▂▁▃▁▂▂▂▂▂▂▂▁▄▂▃▁▂█▂▁▂▂▂▁▁▃▂▂▂▁▂▂</td></tr><tr><td>reward</td><td>▃▆▃▆█▃▅▃▁▁▁▃█▆▆▅█▅▃▆▆▆▅█▅█▆▅▅▅█▁▅▅▃▃▃▅▃▆</td></tr><tr><td>reward_100</td><td>▅▅█▇▅▂▄▄▁▁▃▄▃▂▄▅▇▅▄▄▅▅▄▃▂▃▂▃▃▃▆▆▅▅▆▁▄▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.85342</td></tr><tr><td>loss</td><td>0.10708</td></tr><tr><td>reward</td><td>2</td></tr><tr><td>reward_100</td><td>2.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-sea-2</strong> at: <a href='https://wandb.ai/joanlafuente/Breakout/runs/lxbjyxlr' target=\"_blank\">https://wandb.ai/joanlafuente/Breakout/runs/lxbjyxlr</a><br/> View project at: <a href='https://wandb.ai/joanlafuente/Breakout' target=\"_blank\">https://wandb.ai/joanlafuente/Breakout</a><br/>Synced 2 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241111_181155-lxbjyxlr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lxbjyxlr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jlafuente/Desktop/Project-Paradigms/wandb/run-20241111_183007-56v614c1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joanlafuente/Breakout/runs/56v614c1' target=\"_blank\">ethereal-breeze-3</a></strong> to <a href='https://wandb.ai/joanlafuente/Breakout' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joanlafuente/Breakout' target=\"_blank\">https://wandb.ai/joanlafuente/Breakout</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joanlafuente/Breakout/runs/56v614c1' target=\"_blank\">https://wandb.ai/joanlafuente/Breakout/runs/56v614c1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/joanlafuente/Breakout/runs/56v614c1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x15b442f60>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"Breakout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160)\n",
      "MaxAndSkipObservation: (210, 160)\n",
      "ResizeObservation    : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frame:930 | Total games:15 | Mean reward: 2.300  (epsilon used: 0.99): : 0it [00:02, ?it/s]\n",
      "Frame:10211 | Total games:158 | Mean reward: 1.300  (epsilon used: 0.86): : 0it [00:48, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m<\u001b[39m EXPERIENCE_REPLAY_SIZE:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m states, actions, rewards, dones, next_states, weights \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m states, actions, rewards, dones, next_states, weights \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mto(device), actions\u001b[38;5;241m.\u001b[39mto(device), rewards\u001b[38;5;241m.\u001b[39mto(device), dones\u001b[38;5;241m.\u001b[39mto(device), next_states\u001b[38;5;241m.\u001b[39mto(device), weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(net, target_net, states, actions, rewards, dones, next_states, gamma\u001b[38;5;241m=\u001b[39mGAMMA, criterion\u001b[38;5;241m=\u001b[39mcriterion)\n",
      "Cell \u001b[0;32mIn[135], line 31\u001b[0m, in \u001b[0;36mPrioritizedExperienceReplay.sample\u001b[0;34m(self, BATCH_SIZE, alpha, beta, epsilon)\u001b[0m\n\u001b[1;32m     28\u001b[0m weights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mprobabilities[indices]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m beta\n\u001b[1;32m     29\u001b[0m states, actions, rewards, dones, next_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])\n\u001b[0;32m---> 31\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m     33\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(rewards, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = make_env(ENV_NAME)\n",
    "\n",
    "net = DuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    " \n",
    "buffer = PrioritizedExperienceReplay(EXPERIENCE_REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = EPS_START\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "total_rewards = []\n",
    "frame_number = 0  \n",
    "\n",
    "tbar = tqdm()\n",
    "while True:\n",
    "    frame_number += 1\n",
    "    epsilon = max(epsilon * EPS_DECAY, EPS_MIN)\n",
    "\n",
    "    reward = agent.step(net, target_net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(total_rewards[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "        tbar.set_description(f\"Frame:{frame_number} | Total games:{len(total_rewards)} | Mean reward: {mean_reward:.3f}  (epsilon used: {epsilon:.2f})\")\n",
    "        wandb.log({\"epsilon\": epsilon, \"reward_100\": mean_reward, \"reward\": reward}, step=frame_number)\n",
    "\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(f\"SOLVED in {frame_number} frames and {len(total_rewards)} games\")\n",
    "            break\n",
    "\n",
    "    if len(buffer) < EXPERIENCE_REPLAY_SIZE:\n",
    "        continue\n",
    "\n",
    "    states, actions, rewards, dones, next_states, weights = buffer.sample(BATCH_SIZE)\n",
    "    states, actions, rewards, dones, next_states, weights = states.to(device), actions.to(device), rewards.to(device), dones.to(device), next_states.to(device), weights.to(device)\n",
    "    \n",
    "    loss = compute_loss(net, target_net, states, actions, rewards, dones, next_states, gamma=GAMMA, criterion=criterion)\n",
    "    loss = (loss * weights).mean()\n",
    "    wandb.log({\"loss\": loss.item()}, step=frame_number)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if frame_number % SYNC_TARGET_NETWORK == 0:\n",
    "        target_net.load_state_dict(net.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
